---
title: "evaluation-methodology"
description: "Methods for evaluating AI model outputs - exact match, semantic similarity, LLM-as-judge, comparative evaluation, ELO ranking. Use when measuring AI quality, building eval pipelines, or comparing models."
icon: "microchip"
---

<Info>
  **Category:** Ai-engineering

  **Auto-Detection:** OMGKIT automatically detects when this skill is needed based on your project files.
</Info>

## Overview

Methods for evaluating AI model outputs - exact match, semantic similarity, LLM-as-judge, comparative evaluation, ELO ranking. Use when measuring AI quality, building eval pipelines, or comparing models.

## What You Get

When this skill is active, agents automatically apply:

<Check>Industry best practices</Check>
<Check>Idiomatic patterns</Check>
<Check>Security considerations</Check>
<Check>Performance optimizations</Check>


# Evaluation Methodology

Methods for evaluating Foundation Model outputs.

## Evaluation Approaches

### 1. Exact Evaluation

| Method | Use Case | Example |
|--------|----------|---------|
| Exact Match | QA, Math | `"5" == "5"` |
| Functional Correctness | Code | Pass test cases |
| BLEU/ROUGE | Translation | N-gram overlap |
| Semantic Similarity | Open-ended | Embedding cosine |

```python
# Semantic Similarity
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity

model = SentenceTransformer('all-MiniLM-L6-v2')
emb1 = model.encode([generated])
emb2 = model.encode([reference])
similarity = cosine_similarity(emb1, emb2)[0][0]
```

### 2. AI as Judge

```python
JUDGE_PROMPT = """Rate the response on a scale of 1-5.

Criteria:
- Accuracy: Is information correct?
- Helpfulness: Does it address the need?
- Clarity: Is it easy to understand?

Query: {query}
Response: {response}

Return JSON: {"score": N, "reasoning": "..."}"""

# Multi-judge for reliability
judges = ["gpt-4", "claude-3"]
scores = [get_score(judge, response) for judge in judges]
final_score = sum(scores) / len(scores)
```

### 3. Comparative Evaluation (ELO)

```python
COMPARE_PROMPT = """Compare these responses.

Query: {query}
A: {response_a}
B: {response_b}

Which is better? Return: A, B, or tie"""

def update_elo(rating_a, rating_b, winner, k=32):
    expected_a = 1 / (1 + 10**((rating_b - rating_a) / 400))
    score_a = 1 if winner == "A" else 0 if winner == "B" else 0.5
    return rating_a + k * (score_a - expected_a)
```

## Evaluation Pipeline

```
1. Define Criteria (accuracy, helpfulness, safety)
   ↓
2. Create Scoring Rubric with Examples
   ↓
3. Select Methods (exact + AI judge + human)
   ↓
4. Create Evaluation Dataset
   ↓
5. Run Evaluation
   ↓
6. Analyze & Iterate
```

## Best Practices

1. Use multiple evaluation methods
2. Calibrate AI judges with human data
3. Include both automatic and human evaluation
4. Version your evaluation datasets
5. Track metrics over time
6. Test for position bias in comparisons


## Usage Graph

### Used By Agents

| Agent | Description |
|-------|-------------|
| [ml-engineer](/agents/ml-engineer) | Machine learning engineering specialist for building product... |

### Used By Workflows

| Workflow | Description |
|----------|-------------|
| [ai-engineering/fine-tuning](/workflows/fine-tuning) | Fine-tune foundation models for specific tasks |
| [ai-engineering/model-evaluation](/workflows/model-evaluation) | Comprehensive AI model evaluation |
| [ai-engineering/prompt-engineering](/workflows/prompt-engineering) | Optimize prompts for AI applications |
| [ai-engineering/rag-development](/workflows/rag-development) | Build Retrieval-Augmented Generation systems |


## Configuration

You can customize skill behavior in your project config:

```yaml
# .omgkit/config.yaml
skills:
  evaluation-methodology:
    enabled: true
    # Add skill-specific settings here
```

## When This Skill Activates

OMGKIT detects and activates this skill when it finds:

- Relevant file extensions in your project
- Configuration files specific to this technology
- Package dependencies in package.json, requirements.txt, etc.

## Related Skills

<CardGroup cols={2}>
  <Card title="All Skills" icon="brain" href="/skills/overview">
    See all 151 skills
  </Card>
  <Card title="Ai-engineering" icon="microchip" href="/skills/overview#ai-engineering">
    More ai-engineering skills
  </Card>
</CardGroup>
