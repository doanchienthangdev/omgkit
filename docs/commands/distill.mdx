---
title: "/omgoptim:distill"
description: "Knowledge distillation from teacher model to smaller student model for efficient deployment"
icon: "terminal"
---

<Info>
  **Category:** Omgoptim

  **Syntax:** `/omgoptim:distill "[--teacher <path>] [--student_config <config>]"`
</Info>

## Overview

Knowledge distillation from teacher model to smaller student model for efficient deployment

## Quick Start

```bash
/omgoptim:distill "[--teacher
```


# Knowledge Distillation: "[--teacher \<path\>] [--student_config \<config\>]"

Distill knowledge: **"[--teacher \<path\>] [--student_config \<config\>]"**

## Agent
Uses **performance-engineer-agent** for knowledge distillation.

## Parameters
- **teacher**: Path to teacher (large) model
- **student_config**: Path to student model configuration

## Distillation Types

### Response-Based
- Match soft labels
- Temperature scaling
- KL divergence loss

### Feature-Based
- Match intermediate layers
- Attention transfer
- Hidden state matching

### Relation-Based
- Match pairwise relations
- Graph distillation
- Contrastive learning

## Code Template
```python
from omgkit.optimization import KnowledgeDistiller

distiller = KnowledgeDistiller()

# Define student model (smaller)
student = distiller.create_student(
    architecture="mlp",
    hidden_dims=[64, 32],  # Smaller than teacher
    output_dim=10
)

# Distill
distilled_model = distiller.distill(
    teacher_path="models/teacher_large.pt",
    student=student,
    train_data="data/splits/train.parquet",
    temperature=4.0,
    alpha=0.5,  # Weight for distillation loss
    epochs=50
)

# Compare
distiller.compare(
    teacher="models/teacher_large.pt",
    student=distilled_model,
    test_data="data/splits/test.parquet"
)
```

## Hyperparameters
- **Temperature**: Higher = softer labels (typically 2-10)
- **Alpha**: Balance between hard and soft labels
- **Loss**: KL divergence + cross-entropy

## Benefits
- Smaller model size
- Faster inference
- Lower memory
- Maintained accuracy

## Comparison Output
- Size comparison
- Speed comparison
- Accuracy comparison
- Layer-by-layer analysis

## Progress
- [ ] Teacher loaded
- [ ] Student created
- [ ] Distillation training
- [ ] Quality validated
- [ ] Report generated

Transfer knowledge from large to small model efficiently.



## Tools Used

This command uses the following tools:

- **Task** - Enables task capabilities
- **Read** - Enables read capabilities
- **Write** - Enables write capabilities
- **Bash** - Enables bash capabilities
- **Grep** - Enables grep capabilities
- **Glob** - Enables glob capabilities


## Usage Graph

### Triggered By Agents

| Agent | Description |
|-------|-------------|
| [model-optimizer-agent](/agents/model-optimizer-agent) | Expert agent for optimizing ML models through quantization, ... |


## Examples

### Basic Usage

```bash
/omgoptim:distill "your input here"
```

### With Context

```bash
# First, ensure context is loaded
/context:index

# Then run the command
/omgoptim:distill "detailed description"
```

## Tips

<Note>
For best results, be specific in your descriptions and include relevant file paths or context.
</Note>

## Troubleshooting

<AccordionGroup>
  <Accordion title="Command not found">
    Make sure OMGKIT is installed: `npx omgkit --version`
  </Accordion>
  <Accordion title="Unexpected results">
    Try running `/index` first to refresh codebase context.
  </Accordion>
</AccordionGroup>

## Related Commands

<CardGroup cols={2}>
  <Card title="All Commands" icon="terminal" href="/commands/overview">
    See all 169 commands
  </Card>
  <Card title="Omgoptim Commands" icon="terminal" href="/commands/overview#omgoptim">
    More omgoptim commands
  </Card>
</CardGroup>
