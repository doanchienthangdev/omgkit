---
title: "model-evaluation"
description: "Comprehensive AI model evaluation"
icon: "microchip"
---

<Info>
  **Category:** AI Engineering

  **Complexity:** ðŸŸ¡ Medium

  **Estimated Time:** 2-8 hours
</Info>

## Quick Start

```bash
/workflow:model-evaluation "your description here"
```


# Model Evaluation Workflow

## Overview

The Model Evaluation workflow provides a systematic approach to evaluating AI model performance. It covers defining metrics, creating evaluation datasets, running evaluations, and generating comprehensive reports.

## When to Use

- Comparing model options
- Validating model performance
- Regression testing after changes
- A/B testing model versions
- Compliance and audit requirements

## Steps

### Step 1: Strategy Definition
**Agent:** planner
**Command:** `/planning:plan "evaluation strategy"`
**Duration:** 30-60 minutes

Define strategy:
- Identify evaluation goals
- Select evaluation dimensions
- Define success criteria
- Plan evaluation scope

**Output:** Evaluation strategy document

### Step 2: Metrics Selection
**Agent:** researcher
**Duration:** 30-60 minutes

Select metrics:
- Task-specific metrics (accuracy, F1, BLEU, etc.)
- Quality metrics (coherence, relevance)
- Safety metrics (toxicity, bias)
- Performance metrics (latency, cost)

**Output:** Metrics specification

### Step 3: Dataset Creation
**Agent:** fullstack-developer
**Duration:** 1-4 hours

Create evaluation dataset:
- Collect/generate test cases
- Create golden answers
- Include edge cases
- Balance dataset distribution

**Output:** Evaluation dataset

### Step 4: Evaluation Execution
**Agent:** tester
**Command:** `/dev:test`
**Duration:** 1-4 hours

Run evaluations:
- Execute test suite
- Collect results
- Handle errors gracefully
- Track all metrics

**Output:** Raw evaluation results

### Step 5: Analysis
**Agent:** researcher
**Duration:** 30-60 minutes

Analyze results:
- Statistical analysis
- Error analysis
- Performance breakdown
- Comparison with baselines

**Output:** Analysis report

### Step 6: Report Generation
**Agent:** researcher
**Duration:** 30-60 minutes

Generate report:
- Executive summary
- Detailed metrics
- Recommendations
- Visual dashboards

**Output:** Evaluation report

## Quality Gates

- [ ] Evaluation strategy approved
- [ ] Metrics align with goals
- [ ] Dataset is representative
- [ ] All evaluations completed
- [ ] Results statistically significant
- [ ] Report is comprehensive

## Evaluation Metrics

```
Common AI Evaluation Metrics
============================

ACCURACY METRICS:
- Exact Match: Perfect answer match
- F1 Score: Balance of precision/recall
- BLEU/ROUGE: Text similarity
- BERTScore: Semantic similarity

QUALITY METRICS:
- Coherence: Logical consistency
- Relevance: Answer pertinence
- Completeness: Coverage of topic
- Factuality: Correctness of facts

SAFETY METRICS:
- Toxicity: Harmful content
- Bias: Unfair outputs
- Hallucination: Made-up facts
- Prompt Injection: Security

PERFORMANCE METRICS:
- Latency: Response time
- Throughput: Requests/second
- Cost: $ per request
- Token Usage: Efficiency
```

## Report Template

```markdown
# Model Evaluation Report

## Executive Summary
- Model: [name]
- Evaluation Date: [date]
- Overall Score: [X/100]
- Recommendation: [Pass/Fail/Conditional]

## Metrics Summary
| Metric | Score | Target | Status |
|--------|-------|--------|--------|
| Accuracy | X% | Y% | Pass |
| Latency | Xms | Yms | Pass |

## Detailed Analysis
[Analysis details...]

## Recommendations
1. [Recommendation 1]
2. [Recommendation 2]
```

## Tips

- Use diverse test cases
- Include adversarial examples
- Compare with baselines
- Document methodology
- Version control datasets

## Example Usage

```bash
# Evaluate chat model
/workflow:model-evaluation "GPT-4 for customer support with accuracy and safety metrics"

# Compare models
/workflow:model-evaluation "Claude vs GPT-4 for code generation tasks"

# Regression testing
/workflow:model-evaluation "fine-tuned model v2 vs v1 performance comparison"
```

## Related Workflows

- `rag-development` - For RAG system evaluation
- `prompt-engineering` - For prompt optimization
- `fine-tuning` - For model improvement



## Agents Used

This workflow orchestrates the following agents:

- **[@researcher](/agents/researcher)** - Specialized agent for researcher tasks
- **[@planner](/agents/planner)** - Specialized agent for planner tasks
- **[@tester](/agents/tester)** - Specialized agent for tester tasks



## Skills Applied

- **[ai-engineering/ai-system-evaluation](/skills/ai-engineering/ai-system-evaluation)** - Domain expertise
- **[ai-engineering/evaluation-methodology](/skills/ai-engineering/evaluation-methodology)** - Domain expertise
- **[ai-engineering/dataset-engineering](/skills/ai-engineering/dataset-engineering)** - Domain expertise



## Commands Triggered

- `/planning:plan`
- `/dev:test`



## Prerequisites

- Model to evaluate available
- Evaluation criteria defined


## Orchestration Graph

### Agents Orchestrated

| Agent | Skills | Commands |
|-------|--------|----------|
| [researcher](/agents/researcher) | methodology/research-validation, methodology/brainstorming | /planning:research |
| [planner](/agents/planner) | methodology/writing-plans, methodology/executing-plans... | /planning:plan, /planning:plan-detailed... |
| [tester](/agents/tester) | methodology/test-driven-development, methodology/testing-anti-patterns... | /dev:test, /dev:tdd... |

### Skills Applied

| Skill | Description |
|-------|-------------|
| [ai-engineering/ai-system-evaluation](/skills/ai-system-evaluation) | End-to-end AI system evaluation - model selection, benchmark... |
| [ai-engineering/evaluation-methodology](/skills/evaluation-methodology) | Methods for evaluating AI model outputs - exact match, seman... |
| [ai-engineering/dataset-engineering](/skills/dataset-engineering) | Building and processing datasets - data quality, curation, d... |

### Commands Available

| Command | Description |
|---------|-------------|
| [`/planning:plan`](/commands/planning-plan) | Create implementation plan |
| [`/dev:test`](/commands/dev-test) | Generate and run tests |


## Tips for Best Results

<Note>
Provide detailed context in your workflow description. Include specific requirements, constraints, and expected outcomes for optimal agent performance.
</Note>

## Related Workflows

<CardGroup cols={2}>
  <Card title="All Workflows" icon="diagram-project" href="/workflows/overview">
    See all 29 workflows
  </Card>
  <Card title="AI Engineering" icon="microchip" href="/workflows/overview#ai-engineering">
    More ai-engineering workflows
  </Card>
</CardGroup>
